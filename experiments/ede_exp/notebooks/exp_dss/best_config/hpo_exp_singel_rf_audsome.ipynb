{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Random forest Experiments Serrano/AUDSOME Phase 1\n",
    "Gabriel Iuhasz\n",
    "Anomaly Only\n",
    "Audsome\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer, classification_report, accuracy_score, jaccard_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "import xgboost as xgb\n",
    "# from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
    "from subprocess import check_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%% utils\n"
    }
   },
   "outputs": [],
   "source": [
    "def custom_scoring_reporting(y_pred,\n",
    "                             y,\n",
    "                             definitions,\n",
    "                             prefix):\n",
    "    \"\"\"\n",
    "    Custom function for handling scoring and reporting\n",
    "    :param y_pred: model predictions\n",
    "    :param y: ground truth\n",
    "    :param definitions: variable class definitions (factorize)\n",
    "    :param prefix: prefix to saved files and images\n",
    "    :return: 0\n",
    "    \"\"\"\n",
    "    print(\"Accuracy score is: {}\".format(accuracy_score(y, y_pred)))\n",
    "    print(\"Ballanced accuracy score is: {}\".format(balanced_accuracy_score(y, y_pred)))\n",
    "    print(\"Jaccard score (micro): {}\".format(jaccard_score(y, y_pred, average='micro')))\n",
    "    print(\"Jaccard score (macro): {}\".format(jaccard_score(y, y_pred, average='macro')))\n",
    "    print(\"Jaccard score (weighted): {}\".format(jaccard_score(y, y_pred, average='weighted')))\n",
    "\n",
    "\n",
    "    print(\"Full classification report\")\n",
    "    print(classification_report(y, y_pred, target_names=definitions))\n",
    "    report = classification_report(y, y_pred, output_dict=True)\n",
    "    df_classification_report = pd.DataFrame(report).transpose()\n",
    "    classification_rep_name = \"{}_classification_rep_best.csv\".format(prefix)\n",
    "    df_classification_report.to_csv(os.path.join(model_dir,classification_rep_name), index=False)\n",
    "\n",
    "\n",
    "    print(\"Imbalanced Classification report\")\n",
    "    print(classification_report_imbalanced(y, y_pred, target_names=definitions))\n",
    "    imb_report = classification_report_imbalanced(y, y_pred, target_names=definitions, output_dict=True)\n",
    "    df_imb_classification_report = pd.DataFrame(imb_report).transpose()\n",
    "    classification_imb_rep_name = \"{}_imb_classification_rep_best.csv\".format(prefix)\n",
    "    df_imb_classification_report.to_csv(os.path.join(model_dir,classification_imb_rep_name), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common preprocessing for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Checking files in data location ...\")\n",
    "# train_dir = '/Users/Gabriel/Dropbox/Research/ASPIDE/Datasets/ECI Chaos/Distributed Phase 1/finalized/single_node/training'\n",
    "# train_dir = '/home/gabriel/Research/Aspide/workspace/data_phase'\n",
    "train_dir = '/home/gabriel/research/dipet/serrano/data'\n",
    "print(check_output([\"ls\", train_dir]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Setting paths and datasets\")\n",
    "# Checking if directory exists for data, modells and processed\n",
    "\n",
    "# data_dir = os.path.join(train_dir,'data')\n",
    "model_dir = os.path.join(train_dir,'models')\n",
    "# processed_dir = os.path.join(train_dir,'processed')\n",
    "# if not os.path.exists(data_dir):\n",
    "#     os.makedirs(data_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "# if not os.path.exists(processed_dir):\n",
    "#     os.makedirs(processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_anomaly = pd.read_csv(os.path.join(train_dir,\"df_anomaly.csv\"))\n",
    "df_audsome = pd.read_csv(os.path.join(train_dir,\"df_audsome.csv\"))\n",
    "df_clean = pd.read_csv(os.path.join(train_dir,\"df_clean_single.csv\"))\n",
    "df_clean_audsome = pd.read_csv(os.path.join(train_dir,\"df_clean_ausdome_single.csv\"))\n",
    "\n",
    "# Set index as time\n",
    "df_anomaly.set_index('time', inplace=True)\n",
    "df_audsome.set_index('time', inplace=True)\n",
    "df_clean.set_index('time', inplace=True)\n",
    "df_clean_audsome.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%% Choose dataset to run experiments on\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dataset chosen ...\")\n",
    "data = df_audsome\n",
    "\n",
    "# Nice print\n",
    "nice_y = data['target']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment for removing dummy TODO CHECK if adding back dummy\n",
    "# data.loc[data.target == \"dummy\", 'target'] = \"0\"\n",
    "\n",
    "#Creating the dependent variable class\n",
    "factor = pd.factorize(data['target'])\n",
    "data.target = factor[0]\n",
    "definitions = factor[1]\n",
    "# print(data.target.head())\n",
    "# print(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Splitting dataset into training and ground truth ...\")\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "print(\"Ploting class distribution ..\")\n",
    "pltdist= sns.countplot(nice_y)\n",
    "pltdist.set_xticklabels(pltdist.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Scaling dataset\")\n",
    "# scaler = StandardScaler()\n",
    "# scaler = RobustScaler()\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X_scaled, index=X.index, columns=X.columns) # transform back to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Starting of experiment for RandomForest only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Name of experiment\n",
    "# RandomForest\n",
    "prefix = 'Phase1_RF_evs_audsome'\n",
    "paramgrid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 300, 500, 1000],\n",
    "    \"max_depth\": [5, 10, 25, 50, None],\n",
    "    \"max_features\": [5, 10, 20, 30, 50, 60, 85, 'auto'],\n",
    "    \"min_samples_split\":[2, 5, 11],\n",
    "    \"min_samples_leaf\":[1, 5, 11],\n",
    "    \"criterion\": ['entropy', 'gini'],\n",
    "    \"random_state\": [42]\n",
    "}\n",
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Example of HPO methods https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms/blob/master/HPO_Classification.ipynb\n",
    "# scorer = make_scorer(accuracy_score, )\n",
    "scorer = make_scorer(jaccard_score, average=\"micro\") # TODO check average\n",
    "# scorer = 'accuracy'\n",
    "n_splits = 4 # default 4\n",
    "\n",
    "cv_type = StratifiedKFold(n_splits=n_splits)\n",
    "nj = 40 # Number of jobs\n",
    "\n",
    "cv = EvolutionaryAlgorithmSearchCV(estimator=model,\n",
    "                                   params=paramgrid,\n",
    "                                   scoring=scorer,\n",
    "                                   cv=cv_type, # StratifiedKFold not supported for multilabel-indicator (oh encoding)\n",
    "                                   verbose=4,\n",
    "                                   population_size=40, # 40\n",
    "                                   gene_mutation_prob=0.20,\n",
    "                                   gene_crossover_prob=0.5,\n",
    "                                   tournament_size=4,\n",
    "                                   generations_number=30, #10\n",
    "                                   n_jobs=nj) # for dnn n_jobs must be set to 1 rest is 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove bool values from dict\n",
    "cv.cv_results_.pop('nan_test_score?')\n",
    "\n",
    "print(\"Saving CV results\")\n",
    "file_name = \"{}_hpo_best_cv.csv\".format(prefix)\n",
    "# with open(os.path.join(model_dir,file_name), 'w') as cvfile:\n",
    "#     json.dump(cv.cv_results_, cvfile)\n",
    "cv_test_scores = pd.DataFrame(cv.cv_results_)\n",
    "cv_test_scores.to_csv(os.path.join(model_dir,file_name), index=False)\n",
    "print(\"{} best params: {}\".format(prefix, cv.best_params_))\n",
    "param_name = \"{}_hpo_best_param.json\".format(prefix)\n",
    "with open(os.path.join(model_dir,param_name), 'w') as cvfile:\n",
    "    json.dump(cv.best_params_, cvfile)\n",
    "print(\"{} best score: {}\".format(prefix, cv.best_score_))\n",
    "print(\"Saving best {} estimator\".format(prefix))\n",
    "model_name = \"{}_hpo_best.joblib\".format(prefix)\n",
    "dump(cv.best_estimator_, os.path.join(model_dir,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%% Scoring and reporting for training\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = cv.best_estimator_.predict(X)\n",
    "custom_scoring_reporting(y_pred, y, definitions, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Confusion matrix\")\n",
    "cf_matrix = confusion_matrix(y, y_pred)\n",
    "sns_cf = sns.heatmap(cf_matrix, annot=True, yticklabels=list(definitions), xticklabels=list(definitions))\n",
    "cf_fig = \"{}_cf.png\".format(prefix)\n",
    "sns_cf.figure.savefig(os.path.join(model_dir, cf_fig))\n",
    "\n",
    "# Extract Feature importance\n",
    "feat_importances = pd.Series(cv.best_estimator_.feature_importances_, index=list(data.drop('target', axis=1).columns))\n",
    "featureimp_name = \"{}_hpo_best_featureimp.csv\".format(prefix)\n",
    "feat_importances.to_csv(os.path.join(model_dir, featureimp_name), index=True)\n",
    "# print(feat_importances.head(10))\n",
    "sorted_feature = feat_importances.sort_values(ascending=True)\n",
    "# Plot the feature importances of the forest\n",
    "# plt.figure()\n",
    "plt.figure(figsize=(10,20), dpi=600)\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(X.shape[1]), sorted_feature,\n",
    "       color=\"r\", align=\"center\", )\n",
    "# If you want to define your own labels,\n",
    "# change indices to a list of labels on the following line.\n",
    "plt.yticks(range(X.shape[1]), sorted_feature.index)\n",
    "plt.ylim([-1, X.shape[1]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Scoring on holdout or other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Creating the dependent variable class\n",
    "# factor_h = pd.factorize(df_clean['target'])\n",
    "# df_clean.target = factor_h[0]\n",
    "# definitions_h = factor_h[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Splitting dataset into data and ground truth ...\")\n",
    "# X_h = df_clean.drop('target', axis=1)\n",
    "# y_h = df_clean['target']\n",
    "\n",
    "# Scale\n",
    "# X_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_h = scaler.transform(X_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# y_pred_h = cv.best_estimator_.predict(X_h)\n",
    "#\n",
    "# custom_scoring_reporting(y_pred_h, y_h, definitions, prefix=\"rf_holdout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%% Scoring and reporting for training\n"
    }
   },
   "outputs": [],
   "source": [
    "# y_pred = cv.best_estimator_.predict(X)\n",
    "# custom_scoring_reporting(y_pred, y, definitions, prefix)\n",
    "\n",
    "# jaccard_score(y_h, y_pred_h, average='micro')\n",
    "# y_pred_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
