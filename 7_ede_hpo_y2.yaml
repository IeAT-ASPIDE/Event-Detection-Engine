Connector:
#  PREndpoint: 194.102.62.155 #hal720m.sage.ieat.ro
  Dask:
    SchedulerEndpoint: local # if not local add DASK schedueler endpoint
    Scale: 3 # Number of workers if local otherwise ignored
    SchedulerPort: 8787 # This is the default point
    EnforceCheck: False # Irrelevant for local
  MPort: 9200 # Moitoring port
  KafkaEndpoint: 10.9.8.136
  KafkaPort: 9092
  KafkaTopic: edetopic
#  Query: { "query": 'node_disk_written_bytes_total[5m]'} # Query for specific metrics
  Query: {"query": '{__name__=~"node.+"}[1m]'}
  MetricsInterval: "1m" # Metrics datapoint interval definition
  QSize: 0
  Index: time
  QDelay: "10s" # Polling period for metrics fetching
  Local: /Users/Gabriel/Dropbox/Research/ASPIDE/Datasets/ECI Chaos/Distributed Phase 1/finalized/single_node/training/df_anomaly.csv # Define the path to the local file for training

Mode:
  Training: True
  Validate: False
  Detect: False

Filter:
  Fillna: True # fill none values with 0
  Dropna: True # delete columns woth none values



Augmentation:
  Scaler: # if not used set to false
    StandardScaler:   # All scalers from scikitlearn
      copy: True
#      with_mean: True
#      with_std: true


# For HPO methods
Training:
  Type: hpo
  HPOMethod: Random  # random, grid, bayesian, tpot, evol
  HPOParam:
    n_iter: 2
    n_jobs: -1
    refit: F1_weighted  # if multi metric used, refit should be metric name, mandatory
    verbose: True
  Method: randomforest
  ParamDistribution:
    n_estimators:
      - 10
      - 100
    max_depth:
      - 2
      - 3
  Target: target
  Export: hpo_1
  CV:
    Type: StratifiedKFold  # user defined all from sklearn
    Params:
      n_splits: 5
      shuffle: True
      random_state: 5
  Scorers:
    Scorer_list:
      - Scorer:
          Scorer_name: F1_weighted
          skScorer: f1_weighted
      - Scorer:
          Scorer_name: Jaccard_Index
          skScorer: jaccard_weighted # changes in scoring sklearn, for multiclass add suffix micro, weighted or sample
      - Scorer:
          Scorer_name: AUC
          skScorer: roc_auc_ovr_weighted
    User_scorer1: balanced_accuracy_score

#Training:
#  Type: hpo
#  HPOMethod: Evol  # Random, Grid, Bayesian, tpot, Evol
#  HPOParam:
#    n_jobs: 1 # must be number, not -1 for all
#    scoring: f1_weighted
#    gene_mutation_prob: 0.20
#    gene_crossover_prob: 0.5
#    tournament_size: 4
#    generations_number: 30
#    population_size: 40  # if multi metric used, refit should be metric name, mandatory
#    verbose: 4
#  Method: randomforest
#  ParamDistribution:
#    n_estimators:
#      - 10
#      - 100
#    max_depth:
#      - 2
#      - 3
#  Target: target
#  Export: hpo_1_y2
#  CV:
#    Type: StratifiedKFold  # user defined all from sklearn
#    Params:
#      n_splits: 5
#      shuffle: True
#      random_state: 5
#  Scorers:
#    Scorer_list:
#      - Scorer:
#          Scorer_name: F1_weighted
#          skScorer: f1_weighted
#      - Scorer:
#          Scorer_name: Jaccard_Index
#          skScorer: jaccard_weighted # changes in scoring sklearn, for multiclass add suffix micro, weighted or sample
#      - Scorer:
#          Scorer_name: AUC
#          skScorer: roc_auc_ovr_weighted
#    User_scorer1: balanced_accuracy_score

Detect:
  Method: RandomForest
  Type: classification
  Load: hpo_1
  Scaler: StandardScaler # Same as for training


Misc:
  heap: 512m
  checkpoint: True
  delay: 15s
  interval: 30m
  resetindex: False
  point: False